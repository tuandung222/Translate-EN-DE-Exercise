{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qREBjPLw6pC4"
   },
   "source": [
    ":In this lab, we are going to build a seq2seq model for translating text from English to Catalan without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHtee5wrHchx"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MXK4fuBec6o"
   },
   "source": [
    "To ensure reproducibility of the experiments, we can set the seed to a fixed number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZyyZByjHchy"
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)\n",
    "_ = torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I68eqRSDHchz"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention, xtitle=\"Keys\", ytitle=\"Queries\"):\n",
    "    \"\"\" Plots the attention map.\"\"\"\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(12, 8)})\n",
    "    ax = sns.heatmap(\n",
    "        attention.detach().cpu(),\n",
    "        linewidth=0.5,\n",
    "        cmap=\"Blues\",\n",
    "        square=True)\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    ax.set_xlabel(xtitle)\n",
    "    ax.set_ylabel(ytitle)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1759073517253,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "mRzLycxKHciC",
    "outputId": "a9df9cea-5a6e-4a6a-e62a-f9c39c67ed05"
   },
   "outputs": [],
   "source": [
    "!wget http://www.manythings.org/anki/cat-eng.zip\n",
    "!unzip cat-eng.zip && rm cat-eng.zip\n",
    "!mkdir data\n",
    "!mv \"cat.txt\" \"data/eng-cat.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9KTbHaCHciD"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBxJSbT9y9jo"
   },
   "source": [
    "## Data pre-processing\n",
    "\n",
    "First we need to pre-process the raw text data. We need to make sure to lowercase all of it, remove all non-letter characters, and limit the maximum lenght of the sentences to MAX_LENGTH. Eventually, we create ```pairs```, a list where each component has a sentence and its translation, both pre-processed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1759073519613,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "iZ9eStIxHciD",
    "outputId": "737a73fc-2342-4aea-deb0-cfb7f1e26759"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "import unicodedata\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Remove extra info\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        new_lines.append(line[0:(line.find('CC-BY')-1)])\n",
    "\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in new_lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'cat', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V29yklurzHIo"
   },
   "source": [
    "We have a total of 1306 pairs of sentences containing 1436 english words and 1773 catalan words. We use a very small dataset to reduce training time, but a larger dataset would be necessary to learn something meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZedNSCNzQ2g"
   },
   "source": [
    "## Building the seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxLIx4S0mUA_"
   },
   "source": [
    "We proceed to build our seq2seq model with attention. First we define our encoder RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1eDA6aFHciF"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # TODO: Define the Embedding matrix (use nn.Embedding)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxm-SsMxZfCz"
   },
   "source": [
    "Now we define our decoder, with an attention mechanism (```self.attn```) built inside. Try to map the code with the following diagram:\n",
    "\n",
    "<p align=\"center\"><br>\n",
    "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/attention_tensor_dance_last.jpeg?raw=true\" class=\"center\" title=\"Decoding with attention\" width=\"800\"/>\n",
    "</p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUcH-nIaHciH"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # Removed attention mechanism\n",
    "        # self.attn = AdditiveAttention(self.hidden_size, self.hidden_size, self.hidden_size // 2)\n",
    "        # self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Only use the hidden state from the GRU for the output\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        # Removed attention computation and combination\n",
    "        # context, attn_weights = self.attn(query=x, key=encoder_outputs, value=encoder_outputs)\n",
    "        # x_w_context = torch.cat((x, context), dim=-1)\n",
    "        # x_w_context = self.attn_combine(x_w_context)\n",
    "\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        # Return None for attention weights as they are no longer computed\n",
    "        return output, hidden, None\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oA4zvAnHciI"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTEsKlF1HciI"
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    # First hidden state used by the decoder is the last encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # We feed the decoder with the whole target sentence (teacher forcing),\n",
    "    # first we append SOS_token\n",
    "    decoder_input = torch.cat([\n",
    "        torch.tensor([[SOS_token]], device=device),\n",
    "        target_tensor[:, :-1]\n",
    "    ], dim=1)\n",
    "\n",
    "\n",
    "    # Modified to not receive attention weights\n",
    "    decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "    loss = criterion(\n",
    "        decoder_output.view(-1, decoder_output.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3SRW_D3HciI"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CktDoKfbHciI"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Random sample of n_iters pairs\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "executionInfo": {
     "elapsed": 161306,
     "status": "ok",
     "timestamp": 1759073796426,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "bEaJggtCHciJ",
    "outputId": "506fb282-6498-4bcf-e8ff-f375ae0088d9"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = DecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 50000, print_every=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnqXv0C3HciJ"
   },
   "outputs": [],
   "source": [
    "# def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "#     with torch.no_grad():\n",
    "#         input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "#         encoder_hidden = encoder.initHidden()\n",
    "\n",
    "#         encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "#         decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "#         decoder_hidden = encoder_hidden\n",
    "\n",
    "\n",
    "\n",
    "#         decoded_words = []\n",
    "#         decoder_attentions = []\n",
    "\n",
    "#         for di in range(max_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             decoder_attentions.append(decoder_attention.squeeze(0).data)\n",
    "#             topv, topi = decoder_output.data.topk(1)\n",
    "#             if topi.squeeze().item() == EOS_token:\n",
    "#                 decoded_words.append('<EOS>')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 decoded_words.append(output_lang.index2word[topi.squeeze().item()])\n",
    "#             decoder_input = topi.detach().squeeze(1)\n",
    "\n",
    "#         decoder_output, decoder_hidden = decoder(\n",
    "#             decoder_input, decoder_hidden, encoder_outputs\n",
    "#         )\n",
    "#         return decoded_words, None\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # (1,1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Cho phép decoder trả (out, hid) hoặc (out, hid, attn)\n",
    "            ret = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            if isinstance(ret, (tuple, list)):\n",
    "                if len(ret) == 3:\n",
    "                    decoder_output, decoder_hidden, _ = ret  # bỏ qua attention\n",
    "                elif len(ret) == 2:\n",
    "                    decoder_output, decoder_hidden = ret\n",
    "                else:\n",
    "                    raise ValueError(f\"Decoder returned {len(ret)} values, expected 2 or 3.\")\n",
    "            else:\n",
    "                raise ValueError(\"Decoder must return (output, hidden) or (output, hidden, attn).\")\n",
    "\n",
    "            # decoder_output shape: (1, 1, vocab_size)\n",
    "            topv, topi = decoder_output.data.topk(1)     # topi: (1,1,1)\n",
    "\n",
    "            # Lấy id để kiểm tra EOS\n",
    "            idx = topi.squeeze().item()                  # scalar\n",
    "            if idx == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx])\n",
    "\n",
    "            # Làm input cho bước sau, giữ shape (1,1)\n",
    "            decoder_input = topi.squeeze(-1).detach()    # (1,1)\n",
    "\n",
    "        return decoded_words, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdP2KuXWLk9e"
   },
   "outputs": [],
   "source": [
    "def sentence_accuracy(pred_tokens, ref_tokens):\n",
    "    # cắt theo độ dài nhỏ hơn\n",
    "    min_len = min(len(pred_tokens), len(ref_tokens))\n",
    "    correct = sum(1 for i in range(min_len) if pred_tokens[i] == ref_tokens[i])\n",
    "    return correct / max(len(ref_tokens), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r85CE2r-HciJ"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Source:', pair[0])\n",
    "        print('Reference:', pair[1])\n",
    "\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words[:-1])\n",
    "\n",
    "        print('Model:', output_sentence)\n",
    "\n",
    "        # tokenized reference and hypothesis\n",
    "        ref_tokens = pair[1].split()\n",
    "        hyp_tokens = output_sentence.split()\n",
    "\n",
    "        # Accuracy\n",
    "        acc = sentence_accuracy(hyp_tokens, ref_tokens)\n",
    "        # BLEU (nghiêng về n-gram, nên dùng smoothing cho câu ngắn)\n",
    "        bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothie)\n",
    "\n",
    "        print(f'Accuracy: {acc:.2f}, BLEU: {bleu:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARbjLn6EHciK",
    "outputId": "9953aff9-c419-4652-c0d0-0fa955b2dbe9"
   },
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
