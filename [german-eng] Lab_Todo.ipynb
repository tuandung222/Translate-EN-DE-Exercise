{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq7BHzmk7POZ"
   },
   "source": [
    "## Evaluation of Attention vs. Non-Attention Models on German-English dataset\n",
    "\n",
    "Based on the two sample reports provided earlier, use the new bilingual dataset to carry out the following tasks:\n",
    "\n",
    "1. **Model without Attention**\n",
    "   - Implement a baseline sequence-to-sequence model (encoder + decoder) without any attention mechanism.\n",
    "   - Train the model on the dataset and record the performance.\n",
    "   - Report both token-level accuracy and BLEU score.\n",
    "\n",
    "2. **Model with Attention**\n",
    "   - Implement a sequence-to-sequence model with an attention mechanism.\n",
    "   - Train the model on the same dataset under comparable conditions.\n",
    "   - Report both token-level accuracy and BLEU score.\n",
    "\n",
    "3. **Comparison and Analysis**\n",
    "   - Present the results of both models in a comparative table (accuracy and BLEU).\n",
    "   - Provide a short discussion analyzing:\n",
    "     - How attention influences translation quality.\n",
    "     - Strengths and weaknesses of each approach.\n",
    "     - Which type of sentences (short vs. long, simple vs. complex) benefit most from attention.\n",
    "\n",
    "4. **Deliverables (optional)**\n",
    "   - A Markdown or PDF report containing:\n",
    "     - Model descriptions\n",
    "     - Training setup (hyperparameters, epochs, etc.)\n",
    "     - Quantitative results\n",
    "     - Comparative discussion\n",
    "   - Optionally, include a few qualitative translation examples showing differences between the two models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujLDaVowbzyo"
   },
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10384,
     "status": "ok",
     "timestamp": 1759569515980,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "dTECbsVn6zhw"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1759569516109,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "wJTJLf_-95BX"
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)\n",
    "_ = torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1759569516144,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "1Sl7j7G_96w2"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention, xtitle=\"Keys\", ytitle=\"Queries\"):\n",
    "    \"\"\" Plots the attention map.\"\"\"\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(12, 8)})\n",
    "    ax = sns.heatmap(\n",
    "        attention.detach().cpu(),\n",
    "        linewidth=0.5,\n",
    "        cmap=\"Blues\",\n",
    "        square=True)\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    ax.set_xlabel(xtitle)\n",
    "    ax.set_ylabel(ytitle)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1759569516157,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "H8BgLD0199nF"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "     Implements the additive attention as proposed in \"Neural Machine Translation by Jointly Learning to Align and Translate\".\n",
    "     Args:\n",
    "         q_dim (int): dimesion of the queries\n",
    "         k_dim (int): dimesion of the keys\n",
    "         attn_dim (int): dimension of intermediate vectors\n",
    "\n",
    "     Inputs: query, key, value\n",
    "        query (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
    "        key (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
    "        value (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Result of the Attention Mechanism  (... x T_q x d_v)\n",
    "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, q_dim: int, k_dim: int, attn_dim: int) -> None:\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        # TODO: Create projections of queries and keys\n",
    "        self.proj_q = nn.Linear(q_dim, attn_dim, bias=False)\n",
    "        self.proj_k = nn.Linear(k_dim, attn_dim, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.rand(attn_dim).uniform_(-0.1, 0.1))\n",
    "        self.w = nn.Linear(attn_dim, 1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        q_ = self.proj_q(query) # (... x T_q x q_dim) -> (... x T_q x attn_dim)\n",
    "        k_ = self.proj_k(key)   # (... x T_k x k_dim) -> (... x T_k x attn_dim)\n",
    "\n",
    "        # Prepare for Broadcasting Semantics\n",
    "        q_ = q_.unsqueeze(-2)   # (... x T_q x attn_dim) -> (... x T_q x  1  x attn_dim)\n",
    "        k_ = k_.unsqueeze(-3)   # (... x T_k x attn_dim) -> (... x  1  x T_k x attn_dim)\n",
    "\n",
    "        # Sum thanks to Broadcasting Semantics\n",
    "        attn_hid = torch.tanh(q_ + k_ + self.bias) # (... x T_q x  1  x attn_dim) + (... x  1  x T_k x attn_dim) + (attn_dim) -> (... x T_q x T_k x attn_dim)\n",
    "\n",
    "        attn_logits = self.w(attn_hid)        # (... x T_q x T_k x attn_dim) -> (... x T_q x T_k x 1)\n",
    "        attn_logits = attn_logits.squeeze(-1) # (... x T_q x T_k x 1) -> (... x T_q x T_k)\n",
    "\n",
    "        attn_weights = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # TODO: Compute the output of the attention\n",
    "        output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3999,
     "status": "ok",
     "timestamp": 1759569520468,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "U_5jDoPO-EHy",
    "outputId": "47f5ea17-75be-44cd-bd5e-010ebb439c65"
   },
   "outputs": [],
   "source": [
    "!gdown 1CVMenH5xgDsiq9ZaGAFfkFLG5uWDZvdL\n",
    "!mkdir data\n",
    "!mv \"german-english.txt\" \"data/eng-ger.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!cp \"german-english.txt\" \"data/eng-ger.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759569520484,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "pv58xZCt-V-2"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4089,
     "status": "ok",
     "timestamp": 1759569524581,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "Wz6zQSzP-YGq",
    "outputId": "81a45e08-eb25-405d-fcdd-5e03409d3e27"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "import unicodedata\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Remove extra info\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        new_lines.append(line[0:(line.find('CC-BY')-1)])\n",
    "\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in new_lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'ger', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1759569524607,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "sr8hphEbDB4T"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # Removed attention mechanism\n",
    "        # self.attn = AdditiveAttention(self.hidden_size, self.hidden_size, self.hidden_size // 2)\n",
    "        # self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Only use the hidden state from the GRU for the output\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        # Removed attention computation and combination\n",
    "        # context, attn_weights = self.attn(query=x, key=encoder_outputs, value=encoder_outputs)\n",
    "        # x_w_context = torch.cat((x, context), dim=-1)\n",
    "        # x_w_context = self.attn_combine(x_w_context)\n",
    "\n",
    "        output = F.log_softmax(self.out(output), dim=-1)\n",
    "        # Return None for attention weights as they are no longer computed\n",
    "        return output, hidden, None\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759569524611,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "cCrvd2LO-dbU"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # TODO: Define the Embedding matrix (use nn.Embedding)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1759569524626,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "R9pSgoF2-h2v"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1759569524639,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "CWICGy2M-rhz"
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    # First hidden state used by the decoder is the last encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # We feed the decoder with the whole target sentence (teacher forcing),\n",
    "    # first we append SOS_token\n",
    "    decoder_input = torch.cat([\n",
    "        torch.tensor([[SOS_token]], device=device),\n",
    "        target_tensor[:, :-1]\n",
    "    ], dim=1)\n",
    "\n",
    "\n",
    "    # Modified to not receive attention weights\n",
    "    decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "    loss = criterion(\n",
    "        decoder_output.view(-1, decoder_output.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1759569524663,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "NOuS3gho-trc"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1759569524665,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "cDzSfOE4-uQZ"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Random sample of n_iters pairs\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "executionInfo": {
     "elapsed": 209336,
     "status": "ok",
     "timestamp": 1759569734037,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "hIrebsvM-zD6",
    "outputId": "495cf8df-ae71-49fc-d833-da7dfe03ec2e"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = DecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 50000, print_every=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1759569734069,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "s2RCgaubDKkx"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # (1,1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Cho phép decoder trả (out, hid) hoặc (out, hid, attn)\n",
    "            ret = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            if isinstance(ret, (tuple, list)):\n",
    "                if len(ret) == 3:\n",
    "                    decoder_output, decoder_hidden, _ = ret  # bỏ qua attention\n",
    "                elif len(ret) == 2:\n",
    "                    decoder_output, decoder_hidden = ret\n",
    "                else:\n",
    "                    raise ValueError(f\"Decoder returned {len(ret)} values, expected 2 or 3.\")\n",
    "            else:\n",
    "                raise ValueError(\"Decoder must return (output, hidden) or (output, hidden, attn).\")\n",
    "\n",
    "            # decoder_output shape: (1, 1, vocab_size)\n",
    "            topv, topi = decoder_output.data.topk(1)     # topi: (1,1,1)\n",
    "\n",
    "            # Lấy id để kiểm tra EOS\n",
    "            idx = topi.squeeze().item()                  # scalar\n",
    "            if idx == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx])\n",
    "\n",
    "            # Làm input cho bước sau, giữ shape (1,1)\n",
    "            decoder_input = topi.squeeze(-1).detach()    # (1,1)\n",
    "\n",
    "        return decoded_words, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759569734148,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "kg-FqoalEahz"
   },
   "outputs": [],
   "source": [
    "def sentence_accuracy(pred_tokens, ref_tokens):\n",
    "    # cắt theo độ dài nhỏ hơn\n",
    "    min_len = min(len(pred_tokens), len(ref_tokens))\n",
    "    correct = sum(1 for i in range(min_len) if pred_tokens[i] == ref_tokens[i])\n",
    "    return correct / max(len(ref_tokens), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1576,
     "status": "ok",
     "timestamp": 1759569735725,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "_LXfpUGHEdtt"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Source:', pair[0])\n",
    "        print('Reference:', pair[1])\n",
    "\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words[:-1])\n",
    "\n",
    "        print('Model:', output_sentence)\n",
    "\n",
    "        # tokenized reference and hypothesis\n",
    "        ref_tokens = pair[1].split()\n",
    "        hyp_tokens = output_sentence.split()\n",
    "\n",
    "        # Accuracy\n",
    "        acc = sentence_accuracy(hyp_tokens, ref_tokens)\n",
    "        # BLEU (nghiêng về n-gram, nên dùng smoothing cho câu ngắn)\n",
    "        bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothie)\n",
    "\n",
    "        print(f'Accuracy: {acc:.2f}, BLEU: {bleu:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1759569735820,
     "user": {
      "displayName": "Lâm Tùng Đặng",
      "userId": "16984351246148587519"
     },
     "user_tz": -420
    },
    "id": "2mIJzHz5EgIZ",
    "outputId": "cc5b6dcf-8ebb-427f-a562-80f44f9fcf9c"
   },
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
